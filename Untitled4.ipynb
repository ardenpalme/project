{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd63c18-60ba-4dc2-aab1-c702eef97dc0",
   "metadata": {},
   "source": [
    "# Options Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e184ab-554b-4687-950a-1ff2763a14ae",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textrm{d}L_t = \\Big(\\kappa\\left(\\frac{\\theta}{L_t}-L_t\\right)-\\frac{\\gamma^2}{4}\\Big)\\textrm{dt} + \\gamma\\textrm{d}B_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f0e91-a1fe-4f38-ab50-518de4e09c4d",
   "metadata": {},
   "source": [
    "> Calibrating an option pricing model consists in iteratively adjusting the model parameters so that the differences between the prices of liquidly-traded options and the corresponding model prices are minimized.\n",
    "\n",
    "[Zhang, Amici](https://arxiv.org/html/2407.15536v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a389ccc-e268-4997-9466-d51304d581c5",
   "metadata": {},
   "source": [
    ">To tackle this issue, in the spirit of Huge and Savine (2020) we propose a deep differential\n",
    "network (DDN) for the calibration of the Heston model. Our DDN adds a differentiation\n",
    "layer to the typical structure of a deep neural network. This layer is given by the first-order\n",
    "partial derivatives of the network output with respect to some of the input parameters,\n",
    "namely the parameters of the stochastic variance process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d74b9-0999-49ec-8ae2-3f1518c6b8c7",
   "metadata": {},
   "source": [
    "<img src=\"./images/heston_nn.png\" alt=\"heston slv param nn topology\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e609120-fa85-4e6f-9872-fed1274d681a",
   "metadata": {},
   "source": [
    "$\\theta=$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "163677b3-6b18-4755-a165-c88a1355cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e6a5c71-70d1-4645-b087-29816db845d8",
   "metadata": {},
   "source": [
    "<img src=\"./images/heston_prop1.png\" alt=\"heston slv param nn topology\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a78dc30-e039-4edb-a504-aaafca4e206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function is ReLU\n",
    "class HestonNet(nn.Module):\n",
    "    def __init__(self, alpha, input_size, num_hidden_layers, nodes_per_layer):\n",
    "        assert(hidden_layers >=1)\n",
    "        super().__init__() # initialize parent class\n",
    "        self.lr = alpha\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, nodes_per_layer))\n",
    "        \n",
    "        for _ in range(1, num_hidden_layers-1):\n",
    "            self.layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "\n",
    "        self.output = nn.Linear(nodes_per_layer, 1)\n",
    "\n",
    "    def forward_backward(theta : np.ndarray[9]):\n",
    "        # propagate theta through layers\n",
    "        x = torch.from_numpy(theta)\n",
    "        for layer in self.layers:\n",
    "            x = nn.functional.relu(layer(x))\n",
    "        p_hat = self.output(x)\n",
    "        p_hat.backward()\n",
    "        grad_p_hat = x.grad\n",
    "        return (output, grad_p_hat) # get the partial derivative of the 1D output layer 1 w.r.t. heston params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2560fe6-897e-468c-be3e-a58ada75b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def partial_phi(u : complex, theta_H):\n",
    "#     if(theta_H == \"lambda\"):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a4198f-6055-461e-879e-38a7edf73f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_G(theta : np.ndarray[9]):\n",
    "    (kappa, _lambda, sigma, rho, v0, S0, r, Tau, K ) = theta\n",
    "    theta_Hs = {\"kappa\":kappa, \"lambda\":_lambda, \"sigma\": sigma, \"rho\":rho, \"v0\":v0}\n",
    "    \n",
    "    for key, value in theta_Hs:\n",
    "        k = np.log(K)\n",
    "        def h(u):\n",
    "            return np.real((partial_phi(u,theta_H.key) / (1j*u)) * np.exp(-1j*k*u))\n",
    "            \n",
    "        def g(u):\n",
    "            return np.real((partial_phi(u-1j,theta_H.key) / (1j*u)) * np.exp(-1j*k*u))\n",
    "        \n",
    "        term1 = (S0/np.pi) * quad(h, 1e-8, np.inf)[0]\n",
    "        term2 = ((K*np.exp(-r*tau))/np.pi) * quad(g, 1e-8, np.inf)[0]\n",
    "    return term1 - term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4538033-352c-4298-b011-8e8dbb235529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(p_hat : torch.FloatTensor,\n",
    "         p : np.ndarray, \n",
    "         dp_hat : torch.FloatTensor, \n",
    "         dp : np.ndarray, \n",
    "         model, \n",
    "         lambda_reg = 0.01):\n",
    "    #assert dp_hat.shape == dp.shape && p_hat.shape == p.shape\n",
    "    reg_loss = 0\n",
    "    for param in model.parameters():\n",
    "        reg_loss += torch.sum(param ** 2)\n",
    "    return torch.mean((torch.from_numpy(p)-p_hat)**2) + torch.mean((torch.from_numpy(dp)-dp_hat)**2) + (lambda_reg * reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee64baac-7972-4163-b12a-39952e7342de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.001\n",
    "input_size = 9\n",
    "num_hidden_layers = 3\n",
    "nodes_per_layer = 64\n",
    "batch_size = 32\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec4ec817-5f15-4e94-86a2-220632447835",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "def runif(a,b):\n",
    "    return (b - a) * rng.random() + a\n",
    "    \n",
    "# Generate Options Data \n",
    "def generate_thetas(N : int):\n",
    "    ret = np.ndarray((N,9))\n",
    "    for i in range(N):\n",
    "        theta = np.array([\n",
    "            runif(0.005,5), # kappa\n",
    "            runif(0,1), # lambda\n",
    "            runif(0.1,1), # sigma\n",
    "            runif(-0.95,0), # rho\n",
    "            runif(0,1), # v0\n",
    "            runif(0,0.10), # r\n",
    "            runif(0.05,1), # Tau\n",
    "            runif(10,6000), # S0\n",
    "            runif(-5,5) # ln(K/S0)\n",
    "        ])\n",
    "        theta[-1] = np.exp(theta[-1]) * theta[-2] # recover strike K\n",
    "        ret[i,:] = theta\n",
    "    return ret\n",
    "\n",
    "X = generate_thetas(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b45540e-6a0c-4faf-875f-e603763311ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:3,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7fe4aa6a-b057-4c99-b6bc-096c66066d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(u : np.array, theta : np.ndarray):\n",
    "    assert theta.shape == (9,)\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    \n",
    "    def d(u):\n",
    "        return np.sqrt(np.power((kappa - 1j * rho * sigma *u),2)+np.power(sigma,2)*(1j*u + np.power(u,2)))\n",
    "        \n",
    "    def g(u):\n",
    "        return (kappa - (1j*rho * sigma *u) - d(u)) / (kappa - (1j*rho * sigma *u) + d(u))\n",
    "\n",
    "    def D(Tau, u):\n",
    "        return ((kappa - (1j*rho * sigma *u) - d(u)) / np.power(sigma,2)) * ((1-np.exp(-d(u)*Tau))/(1-g(u)*np.exp(-d(u)*Tau)))\n",
    "\n",
    "    def C(Tau, u):\n",
    "        return (1j*r*u*Tau) + ((kappa * _lambda)/np.power(sigma,2)) * (\n",
    "                ((kappa - 1j*rho * sigma *u) - d(u))*Tau - 2*np.log((1-g(u)*np.exp(-d(u)*Tau)) / (1-g(u)))\n",
    "            )\n",
    "    return np.exp(C(Tau, u) + D(Tau,u)*v0 + 1j*u*np.log(S0))\n",
    "\n",
    "def psi(v: np.array, theta: np.ndarray, alpha):\n",
    "    assert theta.shape == (9,)\n",
    "    (kappa, _lambda, sigma, rho, v0, r, tau, S0, K ) = theta\n",
    "\n",
    "    return (np.exp(-r*tau) * phi(v-(1j*(alpha+1)), theta))/((alpha**2)+alpha-(v**2)+(1j*((2*alpha)+1)*v))\n",
    "\n",
    "def G(theta : np.ndarray, v_max=100, N=2**9, k0=-5, alpha=1.5): \n",
    "    assert theta.shape == (9,)\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    k = np.log(K)\n",
    "\n",
    "    dv = v_max / N\n",
    "    v = np.arange(0,N) * dv\n",
    "    v[0] = 1e-10\n",
    "\n",
    "    dk = (2 * np.pi) / (N * dv)\n",
    "    k_grid = k0 + (np.arange(N) * dk)\n",
    "    strikes = np.exp(k_grid)\n",
    "    \n",
    "    # prices for theoretical strikes based on FFT grid\n",
    "    call_prices = ((np.exp(-alpha*k_grid))/np.pi) * np.real(np.fft.fft(psi(v, theta, alpha)* np.exp(-1j*k0*v)*dv))\n",
    "    \n",
    "    interp = interp1d(strikes, call_prices, kind='linear', fill_value='extrapolate')\n",
    "    return float(interp(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e6d6121b-1964-4181-ba7a-3d20152c8725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.006780416049409693)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-5 + ((2*np.pi)/(2**9)*(1e-3))*2**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "40f2bb3d-4054-4570-8acc-f4091d639ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3083.12641174556"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(X[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad907d3f-330b-44df-bb50-fb612e5eb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data: replace with your actual numpy arrays\n",
    "theta_train = np.random.randn(1000, 9).astype(np.float32)\n",
    "price_train = np.random.randn(1000, 1).astype(np.float32)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TensorDataset(torch.from_numpy(theta_train),\n",
    "                              torch.from_numpy(price_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba79a16-7770-4212-abde-1ba9a12c7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and loss\n",
    "model = NeuralNetwork(alpha, input_size, num_hidden_layers, nodes_per_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=alpha)\n",
    "criterion = nn.MSELoss()   # or your custom loss\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for theta_batch, price_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(theta_batch)   # uses standard forward method\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(pred, price_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * theta_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
