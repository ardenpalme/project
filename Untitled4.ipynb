{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd63c18-60ba-4dc2-aab1-c702eef97dc0",
   "metadata": {},
   "source": [
    "# Options Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e184ab-554b-4687-950a-1ff2763a14ae",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textrm{d}L_t = \\Big(\\kappa\\left(\\frac{\\theta}{L_t}-L_t\\right)-\\frac{\\gamma^2}{4}\\Big)\\textrm{dt} + \\gamma\\textrm{d}B_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f0e91-a1fe-4f38-ab50-518de4e09c4d",
   "metadata": {},
   "source": [
    "> Calibrating an option pricing model consists in iteratively adjusting the model parameters so that the differences between the prices of liquidly-traded options and the corresponding model prices are minimized.\n",
    "\n",
    "[Zhang, Amici](https://arxiv.org/html/2407.15536v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163677b3-6b18-4755-a165-c88a1355cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe7f3fd-77d4-4ced-80dc-2fbf1e629fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c2595-7691-40fc-8d1e-91b401c06994",
   "metadata": {},
   "source": [
    "## Heston Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27668697-39b9-48e2-9c18-b2862041b07a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fft_integral_approx(f, K, u_max=100, N=2**9, k0=-5, params=None):\n",
    "    du = u_max / N\n",
    "    u = np.arange(0,N) * du\n",
    "    u[0] = 1e-10\n",
    "\n",
    "    # create frequency grid \n",
    "    dk = (2 * np.pi) / (N * du) # angular frequency step\n",
    "    k_grid = k0 + (np.arange(N) * dk)\n",
    "\n",
    "    # FFT approximates the Fourier integral at frequencies k_grid (real part only)\n",
    "    int_approx = np.real(np.fft.fft(f(u,params)*np.exp(-1j*k0*u)*du))\n",
    "    \n",
    "    interp = interp1d(k_grid, int_approx, kind='linear', fill_value='extrapolate')\n",
    "    return float(interp(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1babd51-c6c7-4807-b287-d3bc8ab2329b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def d(u, theta):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    return np.sqrt(np.power((kappa - 1j * rho * sigma *u),2)+np.power(sigma,2)*(1j*u + np.power(u,2)))\n",
    "    \n",
    "def g(u, theta):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    d_u = d(u, theta)\n",
    "    return (kappa - (1j*rho * sigma *u) - d_u) / (kappa - (1j*rho * sigma *u) + d_u)\n",
    "\n",
    "def D(u, theta):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    d_u = d(u, theta)\n",
    "    g_u = g(u, theta)\n",
    "    return (kappa - (1j*rho * sigma *u) - d_u / np.power(sigma,2)) * ((1-np.exp(-d_u*Tau))/(1-g_u*np.exp(-d_u*Tau)))\n",
    "\n",
    "def C(u, theta):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    d_u = d(u, theta)\n",
    "    g_u = g(u, theta)\n",
    "    term2 = ((kappa - 1j*rho * sigma *u) - d_u)*Tau - 2*np.log((1-g_u*np.exp(-d_u*Tau)) / (1-g_u))\n",
    "    return (1j*r*u*Tau) + ((kappa * _lambda)/np.power(sigma,2)) * term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a63a8e-4c1b-4f95-9a3c-1fc809ad911a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def partial_D(u : np.array, theta : np.array, theta_H : str):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, tau, S0, K ) = theta\n",
    "    \n",
    "    # Evaluate at given u \n",
    "    d_u = d(u, theta)\n",
    "    g_u = g(u, theta)\n",
    "    a = np.exp(-tau * d_u)\n",
    "\n",
    "    # Common subexpressions\n",
    "    kappa_minus = kappa - 1j * rho * sigma * u        # κ - iρσu\n",
    "    kappa_minus_d = kappa_minus - d_u                 # κ - iρσu - d\n",
    "    one_minus_ag = 1 - a * g_u\n",
    "    one_plus_ag = 1 + a * g_u\n",
    "    iu = 1j * u\n",
    "    iu_plus_u2 = iu + u ** 2\n",
    "\n",
    "    if theta_H == \"kappa\":\n",
    "        # ∂_kappa D_tau\n",
    "        term2 = tau * a * kappa_minus * (1 - g_u) - (1 - a) * one_plus_ag\n",
    "        num = kappa_minus_d * term2\n",
    "        denom = sigma ** 2 * d_u * one_minus_ag ** 2\n",
    "        return num / denom\n",
    "\n",
    "    elif theta_H == \"rho\":\n",
    "        # ∂_rho D_tau\n",
    "        term1 = iu * kappa_minus_d\n",
    "        term2 = one_plus_ag * (1 - a) - tau * a * (1 - g_u) * kappa_minus\n",
    "        num = term1 * term2\n",
    "        denom = sigma * d_u * one_minus_ag ** 2\n",
    "        return num / denom\n",
    "\n",
    "    elif theta_H == \"sigma\":\n",
    "        # ∂_sigma D_tau\n",
    "        bracket1 = (1 - a) * (kappa * a * g_u + kappa - d_u + d_u * a * g_u)\n",
    "        bracket2 = tau * sigma * a * (1 - g_u) * \\\n",
    "                   (1j * rho * u * kappa_minus - sigma * iu_plus_u2)\n",
    "        bracket = bracket1 - bracket2\n",
    "        num_factor = -iu_plus_u2\n",
    "        denom = sigma * d_u * one_minus_ag ** 2 * (kappa_minus + d_u)\n",
    "        return num_factor * bracket / denom\n",
    "\n",
    "    elif theta_H == \"lambda\":\n",
    "        # ∂_lambda D_tau\n",
    "        return 0\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"theta_H must be 'kappa', 'rho', or 'sigma', or 'lambda'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022fdca5-861b-462a-8ec9-b33af675a646",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def partial_C(u: np.array, theta : np.array, theta_H : str):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, tau, S0, K ) = theta\n",
    "\n",
    "    # Evaluate at given u \n",
    "    d_u = d(u)\n",
    "    g_u = g(u)\n",
    "    a = np.exp(-tau * d_u)\n",
    "\n",
    "    # Common subexpressions\n",
    "    kappa_minus = kappa - 1j * rho * sigma * u          # κ - iρσu\n",
    "    kappa_minus_d = kappa_minus - d_u                   # κ - iρσu - d\n",
    "    kappa_plus_d = kappa_minus + d_u                    # κ - iρσu + d\n",
    "    one_minus_ag = 1 - a * g_u\n",
    "    one_plus_ag = 1 + a * g_u\n",
    "    iu = 1j * u\n",
    "    iu_plus_u2 = iu + u ** 2\n",
    "\n",
    "    N = one_minus_ag / (1-g_u)\n",
    "    M = kappa_minus_d * tau - 2*np.log(N)\n",
    "    \n",
    "    if theta_H == \"kappa\":\n",
    "        term2_num = kappa * kappa_minus_d * (\n",
    "            2 * (1 - a) - tau * d_u * one_minus_ag - tau * a * (1 - g_u) * kappa_minus\n",
    "        )\n",
    "        term2_denom = d_u ** 2 * one_minus_ag\n",
    "        return (lam / sigma ** 2) * (M + term2_num / term2_denom)\n",
    "\n",
    "    elif theta_H == \"rho\":\n",
    "        term_num = 1j * kappa * lam * u * kappa_minus_d * (\n",
    "            tau * (d_u + a * kappa_minus) - 2 * (1 - a) - tau * a * g_u * kappa_plus_d\n",
    "        )\n",
    "        term_denom = sigma * d_u ** 2 * one_minus_ag\n",
    "        return term_num / term_denom\n",
    "\n",
    "    elif theta_H == \"sigma\":\n",
    "        # Assemble the five parts inside the brackets\n",
    "        part1 = -rho * sigma ** 2 * tau * u\n",
    "        part2_num = 4 * kappa * sigma ** 3 * (1 - a) * iu_plus_u2\n",
    "        part2_denom = d_u * one_minus_ag * (1 - g_u) * kappa_plus_d ** 2\n",
    "        part2 = part2_num / part2_denom\n",
    "        part3 = -4 * np.log(N)\n",
    "        part4_num = sigma ** 2 * tau * one_plus_ag * (\n",
    "            1j * rho * u * kappa_minus - sigma * iu_plus_u2\n",
    "        )\n",
    "        part4_denom = d_u * one_minus_ag\n",
    "        part4 = part4_num / part4_denom\n",
    "        part5 = -2 * sigma * tau * kappa_minus_d\n",
    "\n",
    "        bracket = part1 + part2 + part3 + part4 + part5\n",
    "        return (kappa * lam / sigma ** 4) * bracket\n",
    "\n",
    "    elif theta_H == \"lambda\":\n",
    "        return (kappa/(sigma**2))*M\n",
    "        \n",
    "    elif theta_H == \"v0\":\n",
    "        return 0\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"theta_H must be 'kappa', 'rho', or 'sigma'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e223aaa-bed0-4f14-a65a-cb38fe8351e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(u : np.array, theta : np.ndarray):\n",
    "    assert theta.shape == (9,)\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    \n",
    "    return np.exp(C(u, theta) + D(u, theta)*v0 + 1j*u*np.log(S0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e6a5c71-70d1-4645-b087-29816db845d8",
   "metadata": {},
   "source": [
    "<img src=\"./images/heston_prop1.png\" alt=\"heston slv param nn topology\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2560fe6-897e-468c-be3e-a58ada75b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_phi(u : np.array, theta : np.array, theta_H : str):\n",
    "    (kappa, _lambda, sigma, rho, v0, r, tau, S0, K ) = theta\n",
    "\n",
    "    if(theta_H in [\"kappa\", \"lambda\", \"sigma\", \"rho\"]):\n",
    "        return phi(u, theta) * (partial_C(u, theta, theta_H) + v0 * partial_D(u, theta, theta_H))\n",
    "    elif theta_H == \"v0\":\n",
    "        return phi(u, theta) * partial_D(u, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe4aa6a-b057-4c99-b6bc-096c66066d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(theta : np.ndarray, u_max=100, N=2**9, k0=-5, alpha=1.5): \n",
    "    assert theta.shape == (9,)\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    k = np.log(K)\n",
    "    \n",
    "    def f(u, theta):\n",
    "        return phi(u-1j, theta)/(1j*u)\n",
    "    \n",
    "    def h(u, theta):\n",
    "        return phi(u, theta)/(1j*u)\n",
    "\n",
    "    Pi1 = 0.5 + (1/np.pi)*fft_integral_approx(f, np.log(K), params=theta)\n",
    "    Pi2 = 0.5 + (1/np.pi)*fft_integral_approx(f, np.log(K), params=theta)\n",
    "    \n",
    "    return Pi1 * S0 - K*np.exp(-r*Tau)*Pi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02a4198f-6055-461e-879e-38a7edf73f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_G(theta : np.ndarray[9]) -> np.ndarray:\n",
    "    (kappa, _lambda, sigma, rho, v0, r, Tau, S0, K ) = theta\n",
    "    theta_Hs = [\"kappa\", \"lambda\", \"sigma\", \"rho\", \"v0\"]\n",
    "    \n",
    "    grad = np.zeros(5)\n",
    "    for theta_H in theta_Hs:\n",
    "        k = np.log(K)\n",
    "        def h(u):\n",
    "            return partial_phi(u-1j,theta, theta_H) / (1j*u) \n",
    "            \n",
    "        def g(u):\n",
    "            return partial_phi(u,theta, theta_H) / (1j*u) \n",
    "        \n",
    "        term1 = (S0/np.pi) * fft_integral_approx(h, np.log(K), params=theta)\n",
    "        term2 = ((K*np.exp(-r*tau))/np.pi) * fft_integral_approx(g, np.log(K), params=theta)\n",
    "        grad[i] = term1 - term2\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def grad_G_mat(thetas : np.ndarray) -> np.ndarray:\n",
    "    assert thetas.shape[1] == 9\n",
    "    \n",
    "    ret = np.zeros((thetas.shape[0],5))\n",
    "    for i in range(thetas.shape[0]):\n",
    "        ret[i,:] = grad_G(thetas[i,:])\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78886f72-d451-4f05-a52c-8803ad994c01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec4ec817-5f15-4e94-86a2-220632447835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Options Data \n",
    "def generate_thetas(N : int):\n",
    "    def runif(a,b):\n",
    "        return (b - a) * rng.random() + a\n",
    "    ret = np.ndarray((N,9))\n",
    "    for i in range(N):\n",
    "        theta = np.array([\n",
    "            runif(0.005,5), # kappa\n",
    "            runif(0,1), # lambda\n",
    "            runif(0.1,1), # sigma\n",
    "            runif(-0.95,0), # rho\n",
    "            runif(0,1), # v0\n",
    "            runif(0,0.10), # r\n",
    "            runif(0.05,1), # Tau\n",
    "            runif(10,6000), # S0\n",
    "            runif(-5,5) # ln(K/S0)\n",
    "        ])\n",
    "        theta[-1] = np.exp(theta[-1]) * theta[-2] # recover strike K\n",
    "        ret[i,:] = theta\n",
    "    return ret\n",
    "\n",
    "X = generate_thetas(1000)\n",
    "\n",
    "p = np.ndarray(X.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    p[i,:] = G(X[i,:])\n",
    "\n",
    "X_train, X_test, p_train, p_test = train_test_split(X, p, test_size = 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1e2d2-05e6-4d6e-bf8b-06cf043ae6a8",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b09e6-f573-4633-8949-a7802e45c1b9",
   "metadata": {},
   "source": [
    "<img src=\"./images/heston_nn.png\" alt=\"heston slv param nn topology\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb026b-a5b1-4461-a8a7-47ee5ae6d599",
   "metadata": {},
   "source": [
    ">To tackle this issue, in the spirit of Huge and Savine (2020) we propose a deep differential\n",
    "network (DDN) for the calibration of the Heston model. Our DDN adds a differentiation\n",
    "layer to the typical structure of a deep neural network. This layer is given by the first-order\n",
    "partial derivatives of the network output with respect to some of the input parameters,\n",
    "namely the parameters of the stochastic variance process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a78dc30-e039-4edb-a504-aaafca4e206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function is ReLU\n",
    "class HestonNet(nn.Module):\n",
    "    def __init__(self, alpha, input_size, num_hidden_layers, nodes_per_layer):\n",
    "        assert(num_hidden_layers >=1)\n",
    "        super().__init__() # initialize parent class\n",
    "        self.lr = alpha\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, nodes_per_layer))\n",
    "        \n",
    "        for _ in range(1, num_hidden_layers-1):\n",
    "            self.layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "\n",
    "        self.output = nn.Linear(nodes_per_layer, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4538033-352c-4298-b011-8e8dbb235529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(p_hat : torch.FloatTensor,\n",
    "         p : torch.FloatTensor, \n",
    "         dp_hat : torch.FloatTensor, \n",
    "         dp : torch.FloatTensor, \n",
    "         params, \n",
    "         lambda_reg = 0.01):\n",
    "    assert dp_hat.shape == dp.shape and p_hat.shape == p.shape\n",
    "    \n",
    "    reg_loss = 0\n",
    "    for param in params:\n",
    "        reg_loss += torch.sum(param ** 2)\n",
    "        \n",
    "    return torch.mean((p-p_hat)**2) + torch.mean((dp-dp_hat)**2) + (lambda_reg * reg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9a780-90c3-4326-ae70-4b10ca0e185b",
   "metadata": {},
   "source": [
    "## Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee64baac-7972-4163-b12a-39952e7342de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.001\n",
    "input_size = 9\n",
    "num_hidden_layers = 3\n",
    "nodes_per_layer = 64\n",
    "batch_size = 32\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad907d3f-330b-44df-bb50-fb612e5eb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(),\n",
    "                              torch.from_numpy(p_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ba79a16-7770-4212-abde-1ba9a12c7ba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad_G.<locals>.h() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     14\u001b[39m pred = model(theta_batch)\n\u001b[32m     15\u001b[39m grad_p_hat = torch.autograd.grad(pred, theta_batch,\n\u001b[32m     16\u001b[39m                        grad_outputs=torch.ones_like(pred),\n\u001b[32m     17\u001b[39m                        create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m grad_p_batch = \u001b[43mgrad_G_mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss = calc_loss(pred, theta_batch, grad_p_hat[:,\u001b[32m0\u001b[39m:\u001b[32m5\u001b[39m], torch.tensor(grad_p_batch), model.parameters())\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgrad_G_mat\u001b[39m\u001b[34m(thetas)\u001b[39m\n\u001b[32m     23\u001b[39m ret = np.zeros((thetas.shape[\u001b[32m0\u001b[39m],\u001b[32m5\u001b[39m))\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(thetas.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     ret[i,:] = \u001b[43mgrad_G\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mgrad_G\u001b[39m\u001b[34m(theta)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg\u001b[39m(u):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partial_phi(u,theta, theta_H) / (\u001b[32m1\u001b[39mj*u) \n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m term1 = (S0/np.pi) * \u001b[43mfft_integral_approx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m term2 = ((K*np.exp(-r*tau))/np.pi) * fft_integral_approx(g, np.log(K), params=theta)\n\u001b[32m     16\u001b[39m grad[i] = term1 - term2\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mfft_integral_approx\u001b[39m\u001b[34m(f, K, u_max, N, k0, params)\u001b[39m\n\u001b[32m      8\u001b[39m k_grid = k0 + (np.arange(N) * dk)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# FFT approximates the Fourier integral at frequencies k_grid (real part only)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m int_approx = np.real(np.fft.fft(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m*np.exp(-\u001b[32m1\u001b[39mj*k0*u)*du))\n\u001b[32m     13\u001b[39m interp = interp1d(k_grid, int_approx, kind=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m, fill_value=\u001b[33m'\u001b[39m\u001b[33mextrapolate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(interp(K))\n",
      "\u001b[31mTypeError\u001b[39m: grad_G.<locals>.h() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Create model, optimizer, and loss\n",
    "model = HestonNet(alpha, input_size, num_hidden_layers, nodes_per_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=alpha)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for theta_batch, price_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        theta_batch.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(theta_batch)\n",
    "        grad_p_hat = torch.autograd.grad(pred, theta_batch,\n",
    "                               grad_outputs=torch.ones_like(pred),\n",
    "                               create_graph=True)[0]\n",
    "        \n",
    "        grad_p_batch = grad_G_mat(theta_batch.detach().numpy())\n",
    "        loss = calc_loss(pred, theta_batch, grad_p_hat[:,0:5], torch.tensor(grad_p_batch), model.parameters())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * theta_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e638b7-cf7b-42c7-9c7b-dc1115a9690a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
